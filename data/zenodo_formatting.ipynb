{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f387aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "from astropy import units\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "620c2096",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/belugawhale/Documents/GitHub/wasp39b_paper/scripts/')\n",
    "from utils import pipeline_dictionary\n",
    "\n",
    "pipeline_dict = pipeline_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b32473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_contact(header, author, contact):\n",
    "    head.attrs[u'author'] = u'{}'.format(author)\n",
    "    head.attrs[u'email'] = u'{}'.format(contact)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e5142",
   "metadata": {},
   "source": [
    "## 1.1 Rewriting average traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "be8d64dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = '/Users/belugawhale/Documents/niriss_real/wasp39/zenodo_upload/1_REDUCTION/1.1_EXAMPLE_TRACES/'\n",
    "inputdir = './traces'\n",
    "files = np.sort([i for i in os.listdir(inputdir) if i.endswith('.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "9fa8c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.close()\n",
    "for i in range(len(files)):\n",
    "    initials = files[i].split('_')[0]\n",
    "    name = pipeline_dict[initials]['name']\n",
    "    \n",
    "    output_name = name + '_order_traces.h5'\n",
    "    hf = h5py.File(os.path.join(outputdir, output_name), 'w')\n",
    "    \n",
    "    head = hf.create_dataset('header',(1,))\n",
    "    add_contact(head, pipeline_dict[initials]['author'],\n",
    "                pipeline_dict[initials]['contact'])\n",
    "    head.attrs[u'units_x'] = u'pixel'\n",
    "    head.attrs[u'units_order_1'] = u'pixel'\n",
    "    head.attrs[u'units_order_2'] = u'pixel'\n",
    "    \n",
    "    tab = Table.read(os.path.join(inputdir, files[i]), format='csv')\n",
    "    \n",
    "    hf.create_dataset('x', data=tab['x'])\n",
    "    hf.create_dataset('order_1', data=tab['order1'])\n",
    "    hf.create_dataset('order_2', data=tab['order2'])\n",
    "    \n",
    "    try:\n",
    "        hf.create_dataset('order_3', data=tab['order3'])\n",
    "        head.attrs[u'units_order_3'] = u'pixel'\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721065db",
   "metadata": {},
   "source": [
    "## 2a Rewriting stellar spectra (nirHiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9c9c522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = '/Users/belugawhale/Documents/niriss_real/wasp39/zenodo_upload/2_STELLAR_SPECTRA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "71c9f472",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['/Users/belugawhale/Downloads/ADF_wasp-39_order1_stellar_spectra_v10.npy',\n",
    "         '/Users/belugawhale/Downloads/ADF_wasp-39_order2_stellar_spectra_v10.npy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "dcfb25ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File(os.path.join(outputdir, 'nirHiss_stellar_spectra.h5'), 'w')\n",
    "\n",
    "head = hf.create_dataset('header',(1,))\n",
    "add_contact(head, pipeline_dict['CMADF']['author'],\n",
    "            pipeline_dict['CMADF']['contact'])\n",
    "\n",
    "head.attrs[u'units_time'.format(i+1)] = u'BJD'\n",
    "\n",
    "\n",
    "for i in range(len(files)):\n",
    "    data = np.load(files[i], allow_pickle=True)\n",
    "    \n",
    "    head.attrs[u'units_wavelength_order_{}'.format(i+1)] = u'micron'\n",
    "    head.attrs[u'units_flux_order_{}'.format(i+1)] = u'DN/s'\n",
    "    head.attrs[u'units_flux_err_order_{}'.format(i+1)] = u'DN/s'\n",
    "    \n",
    "    if i == 0:\n",
    "        hf.create_dataset('time', data=d[0]+2400000)\n",
    "        \n",
    "    hf.create_dataset('wavelength_order_{}'.format(i+1), data=data[1])\n",
    "    hf.create_dataset('flux_order_{}'.format(i+1), data=data[2])\n",
    "    hf.create_dataset('flux_err_order_{}'.format(i+1), data=data[3])\n",
    "\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf83d2",
   "metadata": {},
   "source": [
    "## 2b Rewriting stellar spectra (supreme-SPOON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "8df43e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['/Users/belugawhale/Downloads/WASP-39_atoca_spectra_fullres.fits',\n",
    "         '/Users/belugawhale/Downloads/WASP-39_box_spectra_fullres.fits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "ed022c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = ['atoca', 'box']\n",
    "\n",
    "for i in range(len(files)):\n",
    "    hf = h5py.File(os.path.join(outputdir, \n",
    "                                'supreme-SPOON_{}_stellar_spectra.h5'.format(method[i])), \n",
    "                   'w')\n",
    "\n",
    "    \n",
    "    head = hf.create_dataset('header',(1,))\n",
    "    add_contact(head, pipeline_dict['MCR']['author'],\n",
    "                pipeline_dict['MCR']['contact'])\n",
    "\n",
    "    head.attrs[u'units_time'.format(i+1)] = u'BJD'\n",
    "    \n",
    "    hdu = fits.open(files[i])\n",
    "    \n",
    "    for n in range(2):\n",
    "        head.attrs[u'units_wavelength_order_{}'.format(i+1)] = u'micron'\n",
    "        head.attrs[u'units_flux_order_{}'.format(i+1)] = u'electrons'\n",
    "        head.attrs[u'units_flux_err_order_{}'.format(i+1)] = u'electrons'\n",
    "    \n",
    "\n",
    "    hf.create_dataset('time', data=hdu[9].data)\n",
    "        \n",
    "    hf.create_dataset('wavelength_lower_order_1', data=hdu[1].data)\n",
    "    hf.create_dataset('wavelength_upper_order_1', data=hdu[2].data)\n",
    "    hf.create_dataset('flux_order_1', data=hdu[3].data)\n",
    "    hf.create_dataset('flux_err_order_1'.format(i+1), data=hdu[4].data)\n",
    "    \n",
    "    hf.create_dataset('wavelength_lower_order_2', data=hdu[5].data)\n",
    "    hf.create_dataset('wavelength_upper_order_2', data=hdu[6].data)\n",
    "    hf.create_dataset('flux_order_2', data=hdu[7].data)\n",
    "    hf.create_dataset('flux_err_order_2'.format(i+1), data=hdu[8].data)\n",
    "\n",
    "    hf.close()\n",
    "    \n",
    "    hdu.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa1e72",
   "metadata": {},
   "source": [
    "## 3.1 Rewriting white light curve files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "2d51321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = '/Users/belugawhale/Documents/niriss_real/wasp39/zenodo_upload/3_LIGHT_CURVES/3.1_WHITE_LIGHT_CURVES/'\n",
    "inputdir = './wlcs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "30e96868",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = np.sort(os.listdir(inputdir))\n",
    "n = np.array([0, 0, 1, 2, 2, 3, 3, 4, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "9c7f8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.unique(n):\n",
    "    inds = np.where(n==i)[0]\n",
    "    initials = files[inds[0]].split('-')[0]\n",
    "    name = pipeline_dict[initials]['name']\n",
    "    \n",
    "    output_name = name + '_white_light_curves.h5'\n",
    "    hf = h5py.File(os.path.join(outputdir, output_name), 'w')\n",
    "    \n",
    "    head = hf.create_dataset('header',(1,))\n",
    "    add_contact(head, pipeline_dict[initials]['author'],\n",
    "                pipeline_dict[initials]['contact'])\n",
    "    head.attrs[u'units_residuals'.format(i+1)] = u'ppm'\n",
    "    head.attrs[u'units_residuals_err'.format(i+1)] = u'ppm'\n",
    "    head.attrs[u'units_time'.format(i+1)] = u'time from mid-transit, hours'\n",
    "    \n",
    "    for j,fn in enumerate(files[inds]):\n",
    "        d = np.load(os.path.join(inputdir, fn))\n",
    "        \n",
    "        okey = 'order_{}'.format(j+1)\n",
    "        if j==0:\n",
    "            hf.create_dataset('time', data=d[0])\n",
    "        hf.create_dataset('normalized_flux'+okey, data=d[1])\n",
    "        hf.create_dataset('normalized_flux_err'+okey, data=d[2])\n",
    "        hf.create_dataset('residuals'+okey, data=d[3])\n",
    "        hf.create_dataset('residauls_err'+okey, data=d[4])\n",
    "        hf.create_dataset('best_fit_model'+okey, data=d[5])\n",
    "    \n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88f7feb",
   "metadata": {},
   "source": [
    "## 3.2 Rewriting spectroscopic light curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "320a1958",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = '/Users/belugawhale/Documents/niriss_real/wasp39/zenodo_upload/3_LIGHT_CURVES/3.2_SPECTROSCOPIC_LIGHT_CURVES/'\n",
    "inputdir = './spec_lc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "bca51e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.load('../data/spec_lc/NIRISS_WASP-39_ADF_order1_all_models_and_LCs.npy',\n",
    "               allow_pickle=True).item()\n",
    "data2 = np.load('../data/spec_lc/NIRISS_WASP-39_ADF_order2_all_models_and_LCs.npy',\n",
    "               allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "a8a5af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    initials = 'CMADF'\n",
    "    \n",
    "    data = np.load('../data/spec_lc/NIRISS_WASP-39_ADF_order{}_all_models_and_LCs.npy'.format(i+1),\n",
    "               allow_pickle=True).item()\n",
    "    \n",
    "    if i == 0:\n",
    "        error_array = np.load('../data/spec_lc/ADF_wasp-39_order1_stellar_spectra_v10.npy',\n",
    "                              allow_pickle=True)\n",
    "    else:\n",
    "        error_array = np.load('../data/spec_lc/ADF_wasp-39_order2_stellar_spectra_v10.npy',\n",
    "                              allow_pickle=True)\n",
    "\n",
    "    output_name = 'nirHiss_spectroscopic_light_curves_order_{}.h5'.format(i+1)\n",
    "    \n",
    "    hf = h5py.File(os.path.join(outputdir, output_name), 'w')\n",
    "    \n",
    "    head = hf.create_dataset('header',(1,))\n",
    "    add_contact(head, 'Catriona Murray',\n",
    "                'Catriona.Murray@colorado.edu')\n",
    "    \n",
    "    head.attrs[u'units_time'] = u'BJD'\n",
    "    head.attrs[u'units_wavelength'] = u'micron'\n",
    "    \n",
    "    hf.create_dataset('time', data=data['time']+2400000)\n",
    "    \n",
    "    for n, j in enumerate(np.arange(1,len(list(data.keys())[1:]),\n",
    "                                    5,dtype=int)):\n",
    "        \n",
    "        wave = data[list(data.keys())[j]]\n",
    "        \n",
    "        if wave[0].value > 0.5:\n",
    "        \n",
    "            lc = data[list(data.keys())[j+1]]\n",
    "            model = data[list(data.keys())[j+4]]\n",
    "\n",
    "            ind = np.where(error_array[1]>=wave[0].value)[0][-1]\n",
    "            try:\n",
    "                error = err1[3][:,ind]\n",
    "                fnorm = err1[2][:,ind]\n",
    "                errors = (np.sqrt(np.nansum(error))/len(error))/np.nanmax(fnorm)\n",
    "            except:\n",
    "                error = 0\n",
    "\n",
    "            hf.create_dataset('wavelength_channel_{0:04d}'.format(n), \n",
    "                              data=wave[0].value)\n",
    "\n",
    "\n",
    "            hf.create_dataset('light_curve_channel_{0:04d}'.format(n), \n",
    "                              data=lc)\n",
    "\n",
    "            hf.create_dataset('light_curve_error_channel_{0:04d}'.format(n),\n",
    "                              data=np.full(len(lc), errors))\n",
    "\n",
    "\n",
    "            hf.create_dataset('best_fit_model_channel_{0:04d}'.format(n),\n",
    "                              data=model)\n",
    "\n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab892338",
   "metadata": {},
   "source": [
    "## 4 Rewriting transmission spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "a77e2f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = '/Users/belugawhale/Documents/niriss_real/wasp39/zenodo_upload/4_TRANSMISSION_SPECTRA/'\n",
    "inputdir = './ts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "fc0a7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(hf, data, order, author, email):\n",
    "    \n",
    "    head = hf.create_dataset('header',(1,))\n",
    "    head.attrs[u'units_wavelength'] = u'micron'\n",
    "    head.attrs[u'units_wavelength_err'] = u'micron'\n",
    "    head.attrs[u'units_transit_depth'] = u'ppm'\n",
    "    head.attrs[u'units_transit_depth_err'] = u'ppm'\n",
    "    \n",
    "    add_contact(head, author, email)\n",
    "    \n",
    "    hf.create_dataset('wavelength_order_{}'.format(order), \n",
    "                      data=data['wave'])\n",
    "    hf.create_dataset('wavelength_err_order_{}'.format(order),\n",
    "                      data=data['wave_error'])\n",
    "    hf.create_dataset('transit_depth_order_{}'.format(order),\n",
    "                      data=data['dppm'])\n",
    "    hf.create_dataset('transit_depth_err_order_{}'.format(order),\n",
    "                      data=data['dppm_err'])\n",
    "    hf.create_dataset('quality_flags_order_{}'.format(order),\n",
    "                      data=data['quality'])\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "1bef4243",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = np.sort(os.listdir(inputdir))\n",
    "n = np.array([0, 1, 1,1,1, 2, 2, 3, 3, 4, 4, -1, 6, 6])\n",
    "\n",
    "for i in np.unique(n):\n",
    "    if i >= 0:\n",
    "        inds = np.where(n==i)[0]\n",
    "        initials = files[inds[0]].split('-')[0]\n",
    "        name = pipeline_dict[initials]['name']\n",
    "        \n",
    "        for j in range(len(inds)): \n",
    "            \n",
    "            if len(inds) == 1:\n",
    "                R = 300\n",
    "            else:\n",
    "                close = files[inds[j]].split('_')[-1][:-4]\n",
    "                if '100' in close:\n",
    "                    R = 100\n",
    "                elif '300' in close:\n",
    "                    R = 300\n",
    "                elif 'binned2' in close:\n",
    "                    R = 'native'\n",
    "                elif 'highres' in close:\n",
    "                    R = 'pixel'\n",
    "                else:\n",
    "                    R = 300\n",
    "            \n",
    "            output_name = name + '_transmission_spectrum_R_{}.h5'.format(R)\n",
    "            \n",
    "            try:\n",
    "                hf = h5py.File(os.path.join(outputdir, \n",
    "                                        output_name), 'w')\n",
    "                \n",
    "                data = Table.read(os.path.join(inputdir, files[inds[j]]), \n",
    "                                  format='csv', comment='#')\n",
    "\n",
    "                for order in [1,2]:\n",
    "                    write_file(hf, data[data['order']==order], order,\n",
    "                               pipeline_dict[initials]['author'],\n",
    "                               pipeline_dict[initials]['contact'])\n",
    "\n",
    "                hf.close()                \n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e10d2c",
   "metadata": {},
   "source": [
    "## 5.1 Rewriting grid fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "f65aa215",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = '/Users/belugawhale/Documents/niriss_real/wasp39/zenodo_upload/5_THEORY/5.1_BESTFIT_GRID_MODELS/'\n",
    "inputdir = './grid_best/'\n",
    "files = np.sort(os.listdir(inputdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "2deef18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATMO_clear_R300.spec\n",
      "ATMO_cloudy_R300.spec\n",
      "PICASO_clear_R300.spec\n",
      "PICASO_cloudy_R300.spec\n",
      "Phoenix_clear_R300.spec\n",
      "Phoenix_cloudy_R300.spec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9s/jb487nv52hs8n0q5ftmb1lk80000gn/T/ipykernel_77422/1515062228.py:23: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  val = np.float(params[1])\n"
     ]
    }
   ],
   "source": [
    "for fn in files:\n",
    "    print(fn)\n",
    "    \n",
    "    data = np.loadtxt(os.path.join(inputdir, fn))\n",
    "    output_name = fn[:-5] + '.h5'\n",
    "    \n",
    "    hf = h5py.File(os.path.join(outputdir, output_name), 'w')\n",
    "    head = hf.create_dataset('header',(1,))\n",
    "    \n",
    "    add_contact(head, 'Kazumasa Ohno', 'kono2@ucsc.edu')\n",
    "    head.attrs[u'units_wavelength'] = u'micron'\n",
    "    head.attrs[u'units_transit_depth'] = u'ppm'\n",
    "    \n",
    "\n",
    "    with open(os.path.join(inputdir, fn), encoding='utf-8') as inf:\n",
    "        f = inf.readlines()\n",
    "\n",
    "    parameters = f[1][1:-2].split(',')\n",
    "\n",
    "    for i in range(len(parameters)):\n",
    "        params = parameters[i].split('=')\n",
    "        key = params[0].replace(' ', '')\n",
    "        val = np.float(params[1])\n",
    "        head.attrs[u'{}'.format(key)] = val\n",
    "        \n",
    "    hf.create_dataset('wavelength'.format(order), \n",
    "                      data=data[:,0])\n",
    "    hf.create_dataset('transit_depth'.format(order),\n",
    "                      data=data[:,1])\n",
    "        \n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1738d270",
   "metadata": {},
   "source": [
    "## 5.2 Rewriting contribution plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "37df4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = '/Users/belugawhale/Documents/niriss_real/wasp39/zenodo_upload/5_THEORY/5.2_CONTRIBUTION_MODELS/'\n",
    "inputdir = './contributions/models/'\n",
    "files = np.sort([i for i in os.listdir(inputdir) if i.endswith('.txt')])[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "32919d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in files:\n",
    "    \n",
    "    data = np.loadtxt(os.path.join(inputdir, fn))\n",
    "    \n",
    "    split = fn.split('x')\n",
    "    output_name = 'model_without_'+split[1][:-2]+'.h5'\n",
    "    \n",
    "    hf = h5py.File(os.path.join(outputdir, output_name), 'w')\n",
    "    \n",
    "    head = hf.create_dataset('header',(1,))\n",
    "    add_contact(head, 'Luis Welbanks', 'luis.welbanks@asu.edu')\n",
    "    \n",
    "    head.attrs[u'units_wavelength'] = u'micron'\n",
    "    head.attrs[u'units_transit_depth'] = u'ppm'\n",
    "    \n",
    "    hf.create_dataset('wavelength'.format(order), \n",
    "                      data=data[:,0])\n",
    "    hf.create_dataset('transit_depth'.format(order),\n",
    "                      data=data[:,1])\n",
    "    \n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357ecaf",
   "metadata": {},
   "source": [
    "#### Rewriting cross-sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "eefa9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = Table.read('/Users/belugawhale/Documents/GitHub/wasp39b_paper/data/contributions/xsecs/xsections.txt',\n",
    "                   format='csv', delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "afc69b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File(os.path.join(outputdir, 'species_cross_sections.h5'), 'w')\n",
    "    \n",
    "head = hf.create_dataset('header',(1,))\n",
    "add_contact(head, 'Luis Welbanks', 'luis.welbanks@asu.edu')\n",
    "\n",
    "head.attrs[u'units_wavelength'] = u'micron'\n",
    "head.attrs[u'units_xsection'] = u'm^2'\n",
    "\n",
    "hf.create_dataset('wavelength'.format(order), data=table['wavelength'])\n",
    "\n",
    "for col in table.colnames[1:]:\n",
    "    hf.create_dataset(col[1:], data=table[col])\n",
    "    \n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a65843",
   "metadata": {},
   "source": [
    "## 5.4 Rewriting potassium models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "363c56e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = '/Users/belugawhale/Documents/niriss_real/wasp39/zenodo_upload/5_THEORY/5.4_RULEOUT_K_to_O_MODELS/'\n",
    "inputdir = './ko_short_lam/'\n",
    "files = np.sort([i for i in os.listdir(inputdir) if i.endswith('.txt')])[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "40a599ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File(os.path.join(outputdir, 'ScChimera_K_to_O_models.h5'), 'w')\n",
    "    \n",
    "head = hf.create_dataset('header',(1,))\n",
    "add_contact(head, 'Luis Welbanks', 'luis.welbanks@asu.edu')\n",
    "\n",
    "head.attrs[u'units_wavelength'] = u'micron'\n",
    "head.attrs[u'units_transit_depth'] = u'ppm'\n",
    "\n",
    "for i in range(len(files)):\n",
    "    \n",
    "    data = np.loadtxt(os.path.join(inputdir, files[i]))\n",
    "    \n",
    "    val = float(files[i].split('_')[0])\n",
    "    \n",
    "    head.attrs[u'[K/O]_{}'.format(i)] = val\n",
    "    data = np.loadtxt(os.path.join(inputdir, files[i]))\n",
    "    \n",
    "    hf.create_dataset('model_{}_transit_depth'.format(i), data=data[:,1])\n",
    "    \n",
    "hf.create_dataset('wavelength', data=data[:,0])\n",
    "    \n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc4d57",
   "metadata": {},
   "source": [
    "## 5.5 Rewriting rule-out C/O models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "d9dd443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = '/Users/belugawhale/Documents/niriss_real/wasp39/zenodo_upload/5_THEORY/5.5_RULEOUT_C_to_O_MODELS/'\n",
    "inputdir = './model_rule_out_co/'\n",
    "files = np.sort([i for i in os.listdir(inputdir) if i.endswith('.txt')])\n",
    "co_vals = [0.55, 0.70, 0.80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "71bfacb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File(os.path.join(outputdir, 'ScChimera_C_to_O_models.h5'), 'w')\n",
    "    \n",
    "head = hf.create_dataset('header',(1,))\n",
    "add_contact(head, 'Luis Welbanks', 'luis.welbanks@asu.edu')\n",
    "\n",
    "head.attrs[u'units_wavelength'] = u'micron'\n",
    "head.attrs[u'units_transit_depth'] = u'ppm'\n",
    "\n",
    "for i in range(len(co_vals)):\n",
    "    head.attrs[u'C/O_{}'.format(i)] = co_vals[i]\n",
    "\n",
    "    data = np.loadtxt(os.path.join(inputdir, files[i]))\n",
    "    \n",
    "    hf.create_dataset('model_{}_transit_depth'.format(i), data=data[:,1])\n",
    "\n",
    "hf.create_dataset('wavelength', data=data[:,0])\n",
    "\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e1d58",
   "metadata": {},
   "source": [
    "## 5.6 Rewriting rule-out metallicity models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "594ac48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = '/Users/belugawhale/Documents/niriss_real/wasp39/zenodo_upload/5_THEORY/5.6_RULEOUT_METALLICITY_MODELS/'\n",
    "inputdir = './model_rule_out_z/'\n",
    "files = np.sort([i for i in os.listdir(inputdir) if i.endswith('.txt')])\n",
    "z_vals = [0.0, 0.5, 1.0, 1.5, 2.0, 2.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "09ea28db",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File(os.path.join(outputdir, 'ScChimera_metallicity_models.h5'), 'w')\n",
    "    \n",
    "head = hf.create_dataset('header',(1,))\n",
    "add_contact(head, 'Luis Welbanks', 'luis.welbanks@asu.edu')\n",
    "\n",
    "head.attrs[u'units_wavelength'] = u'micron'\n",
    "head.attrs[u'units_transit_depth'] = u'ppm'\n",
    "\n",
    "for i in range(len(z_vals)):\n",
    "    head.attrs[u'[M/H]_{}'.format(i)] = z_vals[i]\n",
    "    data = np.loadtxt(os.path.join(inputdir, files[i]))\n",
    "    \n",
    "    hf.create_dataset('model_{}_transit_depth'.format(i), data=data[:,1])\n",
    "\n",
    "hf.create_dataset('wavelength', data=data[:,0])\n",
    "\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f6ab3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
